{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9713b6c6-f773-4ec4-a2ba-81bfcecb1817","cell_type":"code","source":"import math\nfrom sympy import symbols, sin, cos, exp, ln, log, tan, asin, atan,cot\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nimport random\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\n\nnltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:10:34.957031Z","iopub.execute_input":"2025-03-21T12:10:34.957417Z","iopub.status.idle":"2025-03-21T12:10:34.966881Z","shell.execute_reply.started":"2025-03-21T12:10:34.957385Z","shell.execute_reply":"2025-03-21T12:10:34.965810Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":4},{"id":"e7c4bd75-902e-4b6e-9fe4-ec391409e3cf","cell_type":"markdown","source":"# Dataset Preprocessing","metadata":{}},{"id":"91c35188-c132-43c5-a474-c79a0a77a251","cell_type":"code","source":"class TaylorDataset:\n    def __init__(self, order, functions=None):\n        self.order = order\n        self.functions = functions if functions else self.default_functions()\n        self.vocab_to_int = None\n        self.int_to_vocab = None\n\n    @staticmethod\n    def default_functions():\n        x = symbols('x')\n        return [\n            sin(x), cos(x), exp(x), ln(1 + x), log(1 + x, 10),\n            1 / (1 + x), x ** 2 + x + 1, tan(x), asin(x), atan(x),exp(sin(x)),exp(tan(x)), cot(x),\n            1/(1+x**2), exp(x)*(1+x), exp(x)*(1-x), 1/(1+x)**2, 1/(1-x)**2, 1/(1-x)**3, 1/(1-x**2), log(3+4*x),\n            1/(1+x), 1/(1+x)**2, 1/(1-x), -ln(1-x)\n            \n        ]\n\n    def generate(self):\n        x = symbols('x')\n        data = []\n        for func in self.functions:\n            expansion = func.series(x, 0, self.order + 1).removeO()\n            data.append({\"function\": str(func), \"expansion\": str(expansion)})\n        return pd.DataFrame(data).sample(frac=1, random_state=42, ignore_index=True)\n\n    def tokenize(self, df):\n        # Tokenize both function and expansion strings.\n        # For expansion tokens, add <SOS> at the start and <EOS> at the end.\n        tokens = []\n        for _, row in df.iterrows():\n            tokens.extend(word_tokenize(row['function']))\n            exp_tokens = ['<SOS>'] + word_tokenize(row['expansion']) + ['<EOS>']\n            tokens.extend(exp_tokens)\n\n        counter = Counter(tokens)\n        vocab = sorted(counter, key=counter.get, reverse=True)\n        # Ensure special tokens exist:\n        for special in ['<SOS>', '<EOS>', '<UNK>']:\n            if special not in vocab:\n                vocab.append(special)\n\n        self.vocab_to_int = {token: i for i, token in enumerate(vocab, 1)}\n        self.int_to_vocab = {i: token for token, i in self.vocab_to_int.items()}\n\n        tokenized_data = {\"function_tokens\": [], \"expansion_tokens\": []}\n        for _, row in df.iterrows():\n            func_tokens = [self.vocab_to_int.get(token, self.vocab_to_int[\"<UNK>\"])\n                           for token in word_tokenize(row[\"function\"])]\n            exp_tokens = (['<SOS>'] + word_tokenize(row[\"expansion\"]) + ['<EOS>'])\n            exp_tokens = [self.vocab_to_int.get(token, self.vocab_to_int[\"<UNK>\"]) for token in exp_tokens]\n            tokenized_data[\"function_tokens\"].append(func_tokens)\n            tokenized_data[\"expansion_tokens\"].append(exp_tokens)\n        return pd.DataFrame(tokenized_data)\n    \n    def get_token_dicts(self):\n        return self.vocab_to_int, self.int_to_vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:08:38.634611Z","iopub.execute_input":"2025-03-21T12:08:38.635076Z","iopub.status.idle":"2025-03-21T12:08:38.649534Z","shell.execute_reply.started":"2025-03-21T12:08:38.635046Z","shell.execute_reply":"2025-03-21T12:08:38.647972Z"}},"outputs":[],"execution_count":2},{"id":"407ed78d-9e9b-48c1-9419-be1a03e775c8","cell_type":"code","source":"order = 4\ntaylor_dataset = TaylorDataset(order)\ndf = taylor_dataset.generate()\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:10:40.673887Z","iopub.execute_input":"2025-03-21T12:10:40.674276Z","iopub.status.idle":"2025-03-21T12:10:41.502722Z","shell.execute_reply.started":"2025-03-21T12:10:40.674245Z","shell.execute_reply":"2025-03-21T12:10:41.501210Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"              function                                          expansion\n0              asin(x)                                         x**3/6 + x\n1        (x + 1)**(-2)                 5*x**4 - 4*x**3 + 3*x**2 - 2*x + 1\n2               sin(x)                                        -x**3/6 + x\n3            1/(1 - x)                         x**4 + x**3 + x**2 + x + 1\n4          exp(tan(x))                 3*x**4/8 + x**3/2 + x**2/2 + x + 1\n5              atan(x)                                        -x**3/3 + x\n6         1/(x**2 + 1)                                    x**4 - x**2 + 1\n7               cos(x)                               x**4/24 - x**2/2 + 1\n8        (x + 1)**(-2)                 5*x**4 - 4*x**3 + 3*x**2 - 2*x + 1\n9            1/(x + 1)                         x**4 - x**3 + x**2 - x + 1\n10              exp(x)                  x**4/24 + x**3/6 + x**2/2 + x + 1\n11              cot(x)                               -x**3/45 - x/3 + 1/x\n12      (1 - x)*exp(x)                      -x**4/8 - x**3/3 - x**2/2 + 1\n13          log(x + 1)                      -x**4/4 + x**3/3 - x**2/2 + x\n14  log(x + 1)/log(10)  -x**4/(4*log(10)) + x**3/(3*log(10)) - x**2/(2...\n15        log(4*x + 3)  -64*x**4/81 + 64*x**3/81 - 8*x**2/9 + 4*x/3 + ...\n16       (1 - x)**(-2)                 5*x**4 + 4*x**3 + 3*x**2 + 2*x + 1\n17           1/(x + 1)                         x**4 - x**3 + x**2 - x + 1\n18       (1 - x)**(-3)               15*x**4 + 10*x**3 + 6*x**2 + 3*x + 1\n19         -log(1 - x)                       x**4/4 + x**3/3 + x**2/2 + x\n20              tan(x)                                         x**3/3 + x\n21         exp(sin(x))                           -x**4/8 + x**2/2 + x + 1\n22      (x + 1)*exp(x)          5*x**4/24 + 2*x**3/3 + 3*x**2/2 + 2*x + 1\n23        1/(1 - x**2)                                    x**4 + x**2 + 1\n24        x**2 + x + 1                                       x**2 + x + 1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>function</th>\n      <th>expansion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>asin(x)</td>\n      <td>x**3/6 + x</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(x + 1)**(-2)</td>\n      <td>5*x**4 - 4*x**3 + 3*x**2 - 2*x + 1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sin(x)</td>\n      <td>-x**3/6 + x</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1/(1 - x)</td>\n      <td>x**4 + x**3 + x**2 + x + 1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>exp(tan(x))</td>\n      <td>3*x**4/8 + x**3/2 + x**2/2 + x + 1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>atan(x)</td>\n      <td>-x**3/3 + x</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1/(x**2 + 1)</td>\n      <td>x**4 - x**2 + 1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>cos(x)</td>\n      <td>x**4/24 - x**2/2 + 1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>(x + 1)**(-2)</td>\n      <td>5*x**4 - 4*x**3 + 3*x**2 - 2*x + 1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1/(x + 1)</td>\n      <td>x**4 - x**3 + x**2 - x + 1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>exp(x)</td>\n      <td>x**4/24 + x**3/6 + x**2/2 + x + 1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>cot(x)</td>\n      <td>-x**3/45 - x/3 + 1/x</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>(1 - x)*exp(x)</td>\n      <td>-x**4/8 - x**3/3 - x**2/2 + 1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>log(x + 1)</td>\n      <td>-x**4/4 + x**3/3 - x**2/2 + x</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>log(x + 1)/log(10)</td>\n      <td>-x**4/(4*log(10)) + x**3/(3*log(10)) - x**2/(2...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>log(4*x + 3)</td>\n      <td>-64*x**4/81 + 64*x**3/81 - 8*x**2/9 + 4*x/3 + ...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>(1 - x)**(-2)</td>\n      <td>5*x**4 + 4*x**3 + 3*x**2 + 2*x + 1</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1/(x + 1)</td>\n      <td>x**4 - x**3 + x**2 - x + 1</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>(1 - x)**(-3)</td>\n      <td>15*x**4 + 10*x**3 + 6*x**2 + 3*x + 1</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-log(1 - x)</td>\n      <td>x**4/4 + x**3/3 + x**2/2 + x</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>tan(x)</td>\n      <td>x**3/3 + x</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>exp(sin(x))</td>\n      <td>-x**4/8 + x**2/2 + x + 1</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>(x + 1)*exp(x)</td>\n      <td>5*x**4/24 + 2*x**3/3 + 3*x**2/2 + 2*x + 1</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1/(1 - x**2)</td>\n      <td>x**4 + x**2 + 1</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>x**2 + x + 1</td>\n      <td>x**2 + x + 1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"id":"b3fda5fd-295a-4ed5-a53c-69498ebe3931","cell_type":"code","source":"tokenized_df = taylor_dataset.tokenize(df)\ntokenized_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:10:45.208266Z","iopub.execute_input":"2025-03-21T12:10:45.208652Z","iopub.status.idle":"2025-03-21T12:10:45.259201Z","shell.execute_reply.started":"2025-03-21T12:10:45.208619Z","shell.execute_reply":"2025-03-21T12:10:45.257987Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                      function_tokens  \\\n0                       [31, 2, 4, 3]   \n1       [2, 4, 1, 5, 3, 14, 2, 18, 3]   \n2                       [25, 2, 4, 3]   \n3                 [11, 2, 5, 8, 4, 3]   \n4             [23, 2, 26, 2, 4, 3, 3]   \n5                       [35, 2, 4, 3]   \n6                 [11, 2, 9, 1, 5, 3]   \n7                       [37, 2, 4, 3]   \n8       [2, 4, 1, 5, 3, 14, 2, 18, 3]   \n9                 [11, 2, 4, 1, 5, 3]   \n10                      [23, 2, 4, 3]   \n11                      [38, 2, 4, 3]   \n12       [2, 5, 8, 4, 3, 28, 2, 4, 3]   \n13                [17, 2, 4, 1, 5, 3]   \n14  [17, 2, 4, 1, 5, 3, 43, 2, 13, 3]   \n15              [17, 2, 51, 1, 30, 3]   \n16      [2, 5, 8, 4, 3, 14, 2, 18, 3]   \n17                [11, 2, 4, 1, 5, 3]   \n18      [2, 5, 8, 4, 3, 14, 2, 56, 3]   \n19                [61, 2, 5, 8, 4, 3]   \n20                      [26, 2, 4, 3]   \n21            [23, 2, 25, 2, 4, 3, 3]   \n22       [2, 4, 1, 5, 3, 28, 2, 4, 3]   \n23                [11, 2, 5, 8, 9, 3]   \n24                    [9, 1, 4, 1, 5]   \n\n                                     expansion_tokens  \n0                                    [6, 24, 1, 4, 7]  \n1               [6, 19, 8, 20, 1, 21, 8, 15, 1, 5, 7]  \n2                                    [6, 32, 1, 4, 7]  \n3                 [6, 12, 1, 22, 1, 9, 1, 4, 1, 5, 7]  \n4                [6, 33, 1, 34, 1, 10, 1, 4, 1, 5, 7]  \n5                                    [6, 36, 1, 4, 7]  \n6                              [6, 12, 8, 9, 1, 5, 7]  \n7                             [6, 27, 8, 10, 1, 5, 7]  \n8               [6, 19, 8, 20, 1, 21, 8, 15, 1, 5, 7]  \n9                 [6, 12, 8, 22, 1, 9, 8, 4, 1, 5, 7]  \n10               [6, 27, 1, 24, 1, 10, 1, 4, 1, 5, 7]  \n11                           [6, 39, 8, 40, 1, 41, 7]  \n12                     [6, 29, 8, 16, 8, 10, 1, 5, 7]  \n13                     [6, 42, 1, 16, 8, 10, 1, 4, 7]  \n14  [6, 44, 2, 45, 2, 13, 3, 3, 1, 46, 2, 47, 2, 1...  \n15   [6, 52, 1, 53, 8, 54, 1, 55, 1, 17, 2, 30, 3, 7]  \n16              [6, 19, 1, 20, 1, 21, 1, 15, 1, 5, 7]  \n17                [6, 12, 8, 22, 1, 9, 8, 4, 1, 5, 7]  \n18              [6, 57, 1, 58, 1, 59, 1, 60, 1, 5, 7]  \n19                     [6, 62, 1, 16, 1, 10, 1, 4, 7]  \n20                                   [6, 16, 1, 4, 7]  \n21                      [6, 29, 1, 10, 1, 4, 1, 5, 7]  \n22              [6, 63, 1, 64, 1, 65, 1, 15, 1, 5, 7]  \n23                             [6, 12, 1, 9, 1, 5, 7]  \n24                              [6, 9, 1, 4, 1, 5, 7]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>function_tokens</th>\n      <th>expansion_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[31, 2, 4, 3]</td>\n      <td>[6, 24, 1, 4, 7]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[2, 4, 1, 5, 3, 14, 2, 18, 3]</td>\n      <td>[6, 19, 8, 20, 1, 21, 8, 15, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[25, 2, 4, 3]</td>\n      <td>[6, 32, 1, 4, 7]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[11, 2, 5, 8, 4, 3]</td>\n      <td>[6, 12, 1, 22, 1, 9, 1, 4, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[23, 2, 26, 2, 4, 3, 3]</td>\n      <td>[6, 33, 1, 34, 1, 10, 1, 4, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[35, 2, 4, 3]</td>\n      <td>[6, 36, 1, 4, 7]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>[11, 2, 9, 1, 5, 3]</td>\n      <td>[6, 12, 8, 9, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>[37, 2, 4, 3]</td>\n      <td>[6, 27, 8, 10, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>[2, 4, 1, 5, 3, 14, 2, 18, 3]</td>\n      <td>[6, 19, 8, 20, 1, 21, 8, 15, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>[11, 2, 4, 1, 5, 3]</td>\n      <td>[6, 12, 8, 22, 1, 9, 8, 4, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>[23, 2, 4, 3]</td>\n      <td>[6, 27, 1, 24, 1, 10, 1, 4, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>[38, 2, 4, 3]</td>\n      <td>[6, 39, 8, 40, 1, 41, 7]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>[2, 5, 8, 4, 3, 28, 2, 4, 3]</td>\n      <td>[6, 29, 8, 16, 8, 10, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>[17, 2, 4, 1, 5, 3]</td>\n      <td>[6, 42, 1, 16, 8, 10, 1, 4, 7]</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>[17, 2, 4, 1, 5, 3, 43, 2, 13, 3]</td>\n      <td>[6, 44, 2, 45, 2, 13, 3, 3, 1, 46, 2, 47, 2, 1...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>[17, 2, 51, 1, 30, 3]</td>\n      <td>[6, 52, 1, 53, 8, 54, 1, 55, 1, 17, 2, 30, 3, 7]</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>[2, 5, 8, 4, 3, 14, 2, 18, 3]</td>\n      <td>[6, 19, 1, 20, 1, 21, 1, 15, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>[11, 2, 4, 1, 5, 3]</td>\n      <td>[6, 12, 8, 22, 1, 9, 8, 4, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>[2, 5, 8, 4, 3, 14, 2, 56, 3]</td>\n      <td>[6, 57, 1, 58, 1, 59, 1, 60, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>[61, 2, 5, 8, 4, 3]</td>\n      <td>[6, 62, 1, 16, 1, 10, 1, 4, 7]</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>[26, 2, 4, 3]</td>\n      <td>[6, 16, 1, 4, 7]</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>[23, 2, 25, 2, 4, 3, 3]</td>\n      <td>[6, 29, 1, 10, 1, 4, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>[2, 4, 1, 5, 3, 28, 2, 4, 3]</td>\n      <td>[6, 63, 1, 64, 1, 65, 1, 15, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>[11, 2, 5, 8, 9, 3]</td>\n      <td>[6, 12, 1, 9, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>[9, 1, 4, 1, 5]</td>\n      <td>[6, 9, 1, 4, 1, 5, 7]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"id":"c15e3cb0-59b3-41c0-8976-386d41a068da","cell_type":"markdown","source":"# PyTorch Dataset and Collate Function","metadata":{}},{"id":"6c976b8e-96d9-4c03-85f7-39a8c57a16a6","cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        data = self.dataset.iloc[idx]\n        # function_tokens are encoder input; expansion_tokens are decoder target.\n        function_tensor = torch.tensor(data['function_tokens'], dtype=torch.long)\n        expansion_tensor = torch.tensor(data['expansion_tokens'], dtype=torch.long)\n        return function_tensor, expansion_tensor\n\ndef collate_fn(batch):\n    # batch: list of (src, trg) pairs.\n    src_seqs, trg_seqs = zip(*batch)\n    src_padded = nn.utils.rnn.pad_sequence(src_seqs, batch_first=True, padding_value=0)\n    trg_padded = nn.utils.rnn.pad_sequence(trg_seqs, batch_first=True, padding_value=0)\n    return src_padded, trg_padded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:10:45.575626Z","iopub.execute_input":"2025-03-21T12:10:45.576001Z","iopub.status.idle":"2025-03-21T12:10:45.582750Z","shell.execute_reply.started":"2025-03-21T12:10:45.575971Z","shell.execute_reply":"2025-03-21T12:10:45.581689Z"}},"outputs":[],"execution_count":7},{"id":"17ae186b-ec6c-4227-b0ad-9487032772b5","cell_type":"markdown","source":"# Positional Encoding","metadata":{}},{"id":"5ecffff0-7587-4a69-81e7-b7034c812d4e","cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)  # [max_len, d_model]\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n                             (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)  # even indices\n        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices\n        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        # x: [batch, seq_len, d_model]\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:10:45.837527Z","iopub.execute_input":"2025-03-21T12:10:45.837858Z","iopub.status.idle":"2025-03-21T12:10:45.845302Z","shell.execute_reply.started":"2025-03-21T12:10:45.837832Z","shell.execute_reply":"2025-03-21T12:10:45.843733Z"}},"outputs":[],"execution_count":8},{"id":"ef4a4314-9f84-4c52-889b-c53b58a5b880","cell_type":"markdown","source":"# Transformer Seq2Seq Model","metadata":{}},{"id":"1697fa67-8113-40bb-947f-fdcef79c7e84","cell_type":"code","source":"class TransformerModel(nn.Module):\n    def __init__(self, input_dim, output_dim, embed_size, nhead, num_layers, hidden_size, dropout=0.1):\n        \"\"\"\n        input_dim: size of source vocabulary (including padding idx=0)\n        output_dim: size of target vocabulary (including padding idx=0)\n        embed_size: embedding dimension (d_model)\n        nhead: number of attention heads\n        num_layers: number of encoder and decoder layers\n        hidden_size: dimension of the feedforward network\n        \"\"\"\n        super().__init__()\n        self.embed_size = embed_size\n        \n        self.src_embedding = nn.Embedding(input_dim, embed_size, padding_idx=0)\n        self.tgt_embedding = nn.Embedding(output_dim, embed_size, padding_idx=0)\n        \n        self.pos_encoder = PositionalEncoding(embed_size, dropout)\n        self.pos_decoder = PositionalEncoding(embed_size, dropout)\n        \n        self.transformer = nn.Transformer(d_model=embed_size, nhead=nhead, \n                                          num_encoder_layers=num_layers, \n                                          num_decoder_layers=num_layers, \n                                          dim_feedforward=hidden_size,\n                                          dropout=dropout)\n        self.fc_out = nn.Linear(embed_size, output_dim)\n    \n    def make_tgt_mask(self, tgt):\n        # Generate a square subsequent mask for the target sequence.\n        tgt_seq_len = tgt.shape[1]\n        tgt_mask = torch.triu(torch.ones(tgt_seq_len, tgt_seq_len) == 1, diagonal=1)\n        tgt_mask = tgt_mask.float().masked_fill(tgt_mask, float('-inf'))\n        return tgt_mask\n\n    def forward(self, src, tgt):\n        \"\"\"\n        src: [batch, src_len]\n        tgt: [batch, tgt_len]\n        \"\"\"\n        src_emb = self.src_embedding(src) * math.sqrt(self.embed_size)  # [batch, src_len, embed_size]\n        src_emb = self.pos_encoder(src_emb)  # [batch, src_len, embed_size]\n        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.embed_size)  # [batch, tgt_len, embed_size]\n        tgt_emb = self.pos_decoder(tgt_emb)  # [batch, tgt_len, embed_size]\n        \n        # Transformer expects input shape: [seq_len, batch, embed_size]\n        src_emb = src_emb.transpose(0, 1)\n        tgt_emb = tgt_emb.transpose(0, 1)\n        \n        tgt_mask = self.make_tgt_mask(tgt).to(src.device)\n        \n        output = self.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask)\n        # output: [tgt_len, batch, embed_size]\n        output = output.transpose(0, 1)  # [batch, tgt_len, embed_size]\n        output = self.fc_out(output)     # [batch, tgt_len, output_dim]\n        # Applying log softmax for NLLLoss\n        return nn.functional.log_softmax(output, dim=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:10:46.051232Z","iopub.execute_input":"2025-03-21T12:10:46.051630Z","iopub.status.idle":"2025-03-21T12:10:46.060761Z","shell.execute_reply.started":"2025-03-21T12:10:46.051597Z","shell.execute_reply":"2025-03-21T12:10:46.059576Z"}},"outputs":[],"execution_count":9},{"id":"d945383b-04e1-493b-812e-baabd86e7d0e","cell_type":"markdown","source":"# Training Class for Transformer","metadata":{}},{"id":"e124a48f-85cf-4f8f-81b8-d584e1d112c8","cell_type":"code","source":"class Train:\n    def __init__(self, epoch, batch_size, input_dim, output_dim, embed_size, nhead, num_layers, hidden_size, dropout=0.1):\n        self.epoch = epoch\n        self.batch_size = batch_size\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        self.model = TransformerModel(input_dim, output_dim, embed_size, nhead, num_layers, hidden_size, dropout).to(self.device)\n        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n        self.criterion = nn.NLLLoss(ignore_index=0)\n    \n    def run(self, dataloader):\n        for epoch in range(self.epoch):\n            self.model.train()\n            epoch_loss = 0\n            for src, tgt in dataloader:\n                src, tgt = src.to(self.device), tgt.to(self.device)\n                self.optimizer.zero_grad()\n                # During training, we feed the full target sequence\n                output = self.model(src, tgt[:, :-1])\n                # output: [batch, tgt_len-1, output_dim]\n                # Compare against target tokens shifted by one (i.e. from position 1 onward)\n                output = output.reshape(-1, output.shape[-1])\n                target = tgt[:, 1:].reshape(-1)\n                loss = self.criterion(output, target)\n                loss.backward()\n                self.optimizer.step()\n                epoch_loss += loss.item()\n            if (epoch + 1) % 100 == 0 or epoch == 0:\n                print(f'Epoch {epoch+1} - Loss: {epoch_loss:.4f}')\n    \n    def get_model(self):\n        return self.model.to(\"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:10:46.312526Z","iopub.execute_input":"2025-03-21T12:10:46.312942Z","iopub.status.idle":"2025-03-21T12:10:46.321781Z","shell.execute_reply.started":"2025-03-21T12:10:46.312909Z","shell.execute_reply":"2025-03-21T12:10:46.320442Z"}},"outputs":[],"execution_count":10},{"id":"48b4c7b6-2bb9-4d46-af4c-1b59c555daef","cell_type":"markdown","source":"# Initialize Dataset and Training Setup","metadata":{}},{"id":"47cba5b0-6ee6-4d99-9ebd-f8e7d5a6391a","cell_type":"code","source":"vocab_to_int, int_to_vocab = taylor_dataset.get_token_dicts()\ndataset = TrainDataset(tokenized_df)\ntrain_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n\n# Hyperparameters for Transformer\nepoch = 500\nbatch_size = 1\nembed_size = 32\nnhead = 4\nnum_layers = 2\nhidden_size = 64\ninput_dim = len(vocab_to_int) + 1  # +1 for padding index 0\noutput_dim = len(vocab_to_int) + 1\n\ntrainer = Train(epoch, batch_size, input_dim, output_dim, embed_size, nhead, num_layers, hidden_size, dropout=0.1)\ntrainer.run(train_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:10:46.565349Z","iopub.execute_input":"2025-03-21T12:10:46.565736Z","iopub.status.idle":"2025-03-21T12:14:10.613908Z","shell.execute_reply.started":"2025-03-21T12:10:46.565704Z","shell.execute_reply":"2025-03-21T12:14:10.612807Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Loss: 94.9696\nEpoch 100 - Loss: 4.1046\nEpoch 200 - Loss: 2.1854\nEpoch 300 - Loss: 3.1796\nEpoch 400 - Loss: 2.9792\nEpoch 500 - Loss: 1.7498\n","output_type":"stream"}],"execution_count":11},{"id":"64e33e70-dc6e-47a1-a4aa-0f372da47888","cell_type":"markdown","source":"# Prediction Function for Transformer","metadata":{}},{"id":"a8624369-7286-406a-837c-5ed06b0e1b46","cell_type":"code","source":"def predict_sample(model, src_tensor, vocab_to_int, int_to_vocab, max_len=30):\n    \"\"\"\n    Predicts the output sequence for a given input sequence (src_tensor) using the trained transformer model.\n    Uses greedy decoding.\n    \"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n    src_tensor = src_tensor.unsqueeze(0).to(device)  # [1, src_len]\n    \n    # Encode source\n    with torch.no_grad():\n        src_emb = model.src_embedding(src_tensor) * math.sqrt(model.embed_size)\n        src_emb = model.pos_encoder(src_emb).transpose(0,1)  # [src_len, 1, embed_size]\n        memory = model.transformer.encoder(src_emb)\n    \n    # Start with <SOS>\n    sos_token = vocab_to_int[\"<SOS>\"]\n    eos_token = vocab_to_int[\"<EOS>\"]\n    tgt_indices = [sos_token]\n    \n    for _ in range(max_len):\n        tgt_tensor = torch.tensor(tgt_indices, dtype=torch.long, device=device).unsqueeze(0)  # [1, t]\n        tgt_emb = model.tgt_embedding(tgt_tensor) * math.sqrt(model.embed_size)\n        tgt_emb = model.pos_decoder(tgt_emb).transpose(0,1)  # [t, 1, embed_size]\n        tgt_mask = model.make_tgt_mask(tgt_tensor).to(device)\n        \n        with torch.no_grad():\n            output = model.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n        output = output.transpose(0,1)  # [1, t, embed_size]\n        output = model.fc_out(output)    # [1, t, output_dim]\n        output = nn.functional.log_softmax(output, dim=-1)\n        next_token = output[0, -1].argmax().item()\n        if next_token == eos_token:\n            break\n        tgt_indices.append(next_token)\n    \n    predicted_tokens = [int_to_vocab[idx] for idx in tgt_indices[1:]]\n    return predicted_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:14:10.615318Z","iopub.execute_input":"2025-03-21T12:14:10.615951Z","iopub.status.idle":"2025-03-21T12:14:10.624767Z","shell.execute_reply.started":"2025-03-21T12:14:10.615913Z","shell.execute_reply":"2025-03-21T12:14:10.623536Z"}},"outputs":[],"execution_count":12},{"id":"b0803d02-b249-49a9-adfd-23277428da44","cell_type":"code","source":"trained_model = trainer.get_model()\ntrained_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:14:10.626711Z","iopub.execute_input":"2025-03-21T12:14:10.627083Z","iopub.status.idle":"2025-03-21T12:14:10.655601Z","shell.execute_reply.started":"2025-03-21T12:14:10.627036Z","shell.execute_reply":"2025-03-21T12:14:10.654225Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TransformerModel(\n  (src_embedding): Embedding(67, 32, padding_idx=0)\n  (tgt_embedding): Embedding(67, 32, padding_idx=0)\n  (pos_encoder): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (pos_decoder): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (transformer): Transformer(\n    (encoder): TransformerEncoder(\n      (layers): ModuleList(\n        (0-1): 2 x TransformerEncoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n          )\n          (linear1): Linear(in_features=32, out_features=64, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=64, out_features=32, bias=True)\n          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): TransformerDecoder(\n      (layers): ModuleList(\n        (0-1): 2 x TransformerDecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n          )\n          (multihead_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n          )\n          (linear1): Linear(in_features=32, out_features=64, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=64, out_features=32, bias=True)\n          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (fc_out): Linear(in_features=32, out_features=67, bias=True)\n)"},"metadata":{}}],"execution_count":13},{"id":"9ef83ff7-8b16-4e51-ae70-be019ac76f39","cell_type":"markdown","source":"# Prediction Example","metadata":{}},{"id":"d8d16b53-a0f2-4bc9-abb0-8e5437b887ab","cell_type":"code","source":"# Pick a sample from the dataset (for example, first sample)\nsample_input, sample_target = dataset[0]\npredicted_expansion = predict_sample(trained_model, sample_input, vocab_to_int, int_to_vocab, max_len=30)\nprint(\"Function Tokens (input):\", [int_to_vocab[token] for token in sample_input.tolist()])\nprint(\"Predicted Expansion:\", \" \".join(predicted_expansion))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T12:14:10.656790Z","iopub.execute_input":"2025-03-21T12:14:10.657159Z","iopub.status.idle":"2025-03-21T12:14:10.690796Z","shell.execute_reply.started":"2025-03-21T12:14:10.657127Z","shell.execute_reply":"2025-03-21T12:14:10.689712Z"}},"outputs":[{"name":"stdout","text":"Function Tokens (input): ['asin', '(', 'x', ')']\nPredicted Expansion: x**3/6 + x\n","output_type":"stream"}],"execution_count":14},{"id":"aee1f42c-d719-4bd9-99cb-fa4da3a8bc6c","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a6a4718f-e030-4d9a-96f6-8e991ff72e3c","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}