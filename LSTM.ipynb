{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"6d14bb5c-95f5-4ee1-982b-9286fff373c3","cell_type":"code","source":"from sympy import symbols, sin, cos, exp, ln, log, tan, asin, atan,cot\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nimport random\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\n\nnltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T06:13:40.237673Z","iopub.execute_input":"2025-03-21T06:13:40.237979Z","iopub.status.idle":"2025-03-21T06:13:48.551470Z","shell.execute_reply.started":"2025-03-21T06:13:40.237945Z","shell.execute_reply":"2025-03-21T06:13:48.550388Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"id":"2ce20711-5102-4ad6-b661-59fb8e57ce18","cell_type":"markdown","source":"# Dataset Generation and Tokenization","metadata":{}},{"id":"6edbaca5-d656-4525-a290-2e60d07e8dd2","cell_type":"code","source":"# Dataset Preprocessing\nclass TaylorDataset:\n    def __init__(self, order, functions=None):\n        self.order = order\n        self.functions = functions if functions else self.default_functions()\n        self.vocab_to_int = None\n        self.int_to_vocab = None\n\n    @staticmethod\n    def default_functions():\n        x = symbols('x')\n        return [\n            sin(x), cos(x), exp(x), ln(1 + x), log(1 + x, 10),\n            1 / (1 + x), x ** 2 + x + 1, tan(x), asin(x), atan(x),exp(sin(x)),exp(tan(x)), cot(x),\n            1/(1+x**2), exp(x)*(1+x), exp(x)*(1-x), 1/(1+x)**2, 1/(1-x)**2, 1/(1-x)**3, 1/(1-x**2), log(3+4*x),\n            1/(1+x), 1/(1+x)**2, 1/(1-x), -ln(1-x)\n        ]\n\n    def generate(self):\n        x = symbols('x')\n        data = []\n        for func in self.functions:\n            expansion = func.series(x, 0, self.order + 1).removeO()\n            data.append({\"function\": str(func), \"expansion\": str(expansion)})\n        return pd.DataFrame(data).sample(frac=1, random_state=42, ignore_index=True)\n\n    def tokenize(self, df):\n        # Tokenize both function and expansion strings.\n        # For Taylor expansion tokens, add <SOS> at start and <EOS> at end.\n        tokens = []\n        for _, row in df.iterrows():\n            tokens.extend(word_tokenize(row['function']))\n            # add <SOS> and <EOS> for expansions\n            exp_tokens = ['<SOS>'] + word_tokenize(row['expansion']) + ['<EOS>']\n            tokens.extend(exp_tokens)\n\n        counter = Counter(tokens)\n        vocab = sorted(counter, key=counter.get, reverse=True)\n        # Ensure special tokens exist:\n        for special in ['<SOS>', '<EOS>', '<UNK>']:\n            if special not in vocab:\n                vocab.append(special)\n\n        self.vocab_to_int = {token: i for i, token in enumerate(vocab, 1)}\n        self.int_to_vocab = {i: token for token, i in self.vocab_to_int.items()}\n\n        tokenized_data = {\"function_tokens\": [], \"expansion_tokens\": []}\n\n        for _, row in df.iterrows():\n            func_tokens = [self.vocab_to_int.get(token, self.vocab_to_int[\"<UNK>\"]) \n                           for token in word_tokenize(row[\"function\"])]\n            exp_tokens = (['<SOS>'] + word_tokenize(row[\"expansion\"]) + ['<EOS>'])\n            exp_tokens = [self.vocab_to_int.get(token, self.vocab_to_int[\"<UNK>\"]) for token in exp_tokens]\n            tokenized_data[\"function_tokens\"].append(func_tokens)\n            tokenized_data[\"expansion_tokens\"].append(exp_tokens)\n        \n        return pd.DataFrame(tokenized_data)\n    \n    def get_token_dicts(self):\n        return self.vocab_to_int, self.int_to_vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T06:13:48.552535Z","iopub.execute_input":"2025-03-21T06:13:48.553028Z","iopub.status.idle":"2025-03-21T06:13:48.565839Z","shell.execute_reply.started":"2025-03-21T06:13:48.552981Z","shell.execute_reply":"2025-03-21T06:13:48.564630Z"}},"outputs":[],"execution_count":2},{"id":"a4ba4d90-03d1-4042-8625-57f654ef801d","cell_type":"code","source":"# Initialize Dataset\norder = 4\ntaylor_dataset = TaylorDataset(order)\ndf = taylor_dataset.generate()\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T06:55:13.144120Z","iopub.execute_input":"2025-03-21T06:55:13.144751Z","iopub.status.idle":"2025-03-21T06:55:13.934400Z","shell.execute_reply.started":"2025-03-21T06:55:13.144706Z","shell.execute_reply":"2025-03-21T06:55:13.933196Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"              function                                          expansion\n0              asin(x)                                         x**3/6 + x\n1        (x + 1)**(-2)                 5*x**4 - 4*x**3 + 3*x**2 - 2*x + 1\n2               sin(x)                                        -x**3/6 + x\n3            1/(1 - x)                         x**4 + x**3 + x**2 + x + 1\n4          exp(tan(x))                 3*x**4/8 + x**3/2 + x**2/2 + x + 1\n5              atan(x)                                        -x**3/3 + x\n6         1/(x**2 + 1)                                    x**4 - x**2 + 1\n7               cos(x)                               x**4/24 - x**2/2 + 1\n8        (x + 1)**(-2)                 5*x**4 - 4*x**3 + 3*x**2 - 2*x + 1\n9            1/(x + 1)                         x**4 - x**3 + x**2 - x + 1\n10              exp(x)                  x**4/24 + x**3/6 + x**2/2 + x + 1\n11              cot(x)                               -x**3/45 - x/3 + 1/x\n12      (1 - x)*exp(x)                      -x**4/8 - x**3/3 - x**2/2 + 1\n13          log(x + 1)                      -x**4/4 + x**3/3 - x**2/2 + x\n14  log(x + 1)/log(10)  -x**4/(4*log(10)) + x**3/(3*log(10)) - x**2/(2...\n15        log(4*x + 3)  -64*x**4/81 + 64*x**3/81 - 8*x**2/9 + 4*x/3 + ...\n16       (1 - x)**(-2)                 5*x**4 + 4*x**3 + 3*x**2 + 2*x + 1\n17           1/(x + 1)                         x**4 - x**3 + x**2 - x + 1\n18       (1 - x)**(-3)               15*x**4 + 10*x**3 + 6*x**2 + 3*x + 1\n19         -log(1 - x)                       x**4/4 + x**3/3 + x**2/2 + x\n20              tan(x)                                         x**3/3 + x\n21         exp(sin(x))                           -x**4/8 + x**2/2 + x + 1\n22      (x + 1)*exp(x)          5*x**4/24 + 2*x**3/3 + 3*x**2/2 + 2*x + 1\n23        1/(1 - x**2)                                    x**4 + x**2 + 1\n24        x**2 + x + 1                                       x**2 + x + 1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>function</th>\n      <th>expansion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>asin(x)</td>\n      <td>x**3/6 + x</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(x + 1)**(-2)</td>\n      <td>5*x**4 - 4*x**3 + 3*x**2 - 2*x + 1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sin(x)</td>\n      <td>-x**3/6 + x</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1/(1 - x)</td>\n      <td>x**4 + x**3 + x**2 + x + 1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>exp(tan(x))</td>\n      <td>3*x**4/8 + x**3/2 + x**2/2 + x + 1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>atan(x)</td>\n      <td>-x**3/3 + x</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1/(x**2 + 1)</td>\n      <td>x**4 - x**2 + 1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>cos(x)</td>\n      <td>x**4/24 - x**2/2 + 1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>(x + 1)**(-2)</td>\n      <td>5*x**4 - 4*x**3 + 3*x**2 - 2*x + 1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1/(x + 1)</td>\n      <td>x**4 - x**3 + x**2 - x + 1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>exp(x)</td>\n      <td>x**4/24 + x**3/6 + x**2/2 + x + 1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>cot(x)</td>\n      <td>-x**3/45 - x/3 + 1/x</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>(1 - x)*exp(x)</td>\n      <td>-x**4/8 - x**3/3 - x**2/2 + 1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>log(x + 1)</td>\n      <td>-x**4/4 + x**3/3 - x**2/2 + x</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>log(x + 1)/log(10)</td>\n      <td>-x**4/(4*log(10)) + x**3/(3*log(10)) - x**2/(2...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>log(4*x + 3)</td>\n      <td>-64*x**4/81 + 64*x**3/81 - 8*x**2/9 + 4*x/3 + ...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>(1 - x)**(-2)</td>\n      <td>5*x**4 + 4*x**3 + 3*x**2 + 2*x + 1</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1/(x + 1)</td>\n      <td>x**4 - x**3 + x**2 - x + 1</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>(1 - x)**(-3)</td>\n      <td>15*x**4 + 10*x**3 + 6*x**2 + 3*x + 1</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-log(1 - x)</td>\n      <td>x**4/4 + x**3/3 + x**2/2 + x</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>tan(x)</td>\n      <td>x**3/3 + x</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>exp(sin(x))</td>\n      <td>-x**4/8 + x**2/2 + x + 1</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>(x + 1)*exp(x)</td>\n      <td>5*x**4/24 + 2*x**3/3 + 3*x**2/2 + 2*x + 1</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1/(1 - x**2)</td>\n      <td>x**4 + x**2 + 1</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>x**2 + x + 1</td>\n      <td>x**2 + x + 1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"id":"1bb1e646-7fa5-4704-b146-9441c76bfdb9","cell_type":"code","source":"tokenized_df = taylor_dataset.tokenize(df)\ntokenized_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T06:55:56.651432Z","iopub.execute_input":"2025-03-21T06:55:56.651820Z","iopub.status.idle":"2025-03-21T06:55:56.696330Z","shell.execute_reply.started":"2025-03-21T06:55:56.651784Z","shell.execute_reply":"2025-03-21T06:55:56.695259Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                      function_tokens  \\\n0                       [31, 2, 4, 3]   \n1       [2, 4, 1, 5, 3, 14, 2, 18, 3]   \n2                       [25, 2, 4, 3]   \n3                 [11, 2, 5, 8, 4, 3]   \n4             [23, 2, 26, 2, 4, 3, 3]   \n5                       [35, 2, 4, 3]   \n6                 [11, 2, 9, 1, 5, 3]   \n7                       [37, 2, 4, 3]   \n8       [2, 4, 1, 5, 3, 14, 2, 18, 3]   \n9                 [11, 2, 4, 1, 5, 3]   \n10                      [23, 2, 4, 3]   \n11                      [38, 2, 4, 3]   \n12       [2, 5, 8, 4, 3, 28, 2, 4, 3]   \n13                [17, 2, 4, 1, 5, 3]   \n14  [17, 2, 4, 1, 5, 3, 43, 2, 13, 3]   \n15              [17, 2, 51, 1, 30, 3]   \n16      [2, 5, 8, 4, 3, 14, 2, 18, 3]   \n17                [11, 2, 4, 1, 5, 3]   \n18      [2, 5, 8, 4, 3, 14, 2, 56, 3]   \n19                [61, 2, 5, 8, 4, 3]   \n20                      [26, 2, 4, 3]   \n21            [23, 2, 25, 2, 4, 3, 3]   \n22       [2, 4, 1, 5, 3, 28, 2, 4, 3]   \n23                [11, 2, 5, 8, 9, 3]   \n24                    [9, 1, 4, 1, 5]   \n\n                                     expansion_tokens  \n0                                    [6, 24, 1, 4, 7]  \n1               [6, 19, 8, 20, 1, 21, 8, 15, 1, 5, 7]  \n2                                    [6, 32, 1, 4, 7]  \n3                 [6, 12, 1, 22, 1, 9, 1, 4, 1, 5, 7]  \n4                [6, 33, 1, 34, 1, 10, 1, 4, 1, 5, 7]  \n5                                    [6, 36, 1, 4, 7]  \n6                              [6, 12, 8, 9, 1, 5, 7]  \n7                             [6, 27, 8, 10, 1, 5, 7]  \n8               [6, 19, 8, 20, 1, 21, 8, 15, 1, 5, 7]  \n9                 [6, 12, 8, 22, 1, 9, 8, 4, 1, 5, 7]  \n10               [6, 27, 1, 24, 1, 10, 1, 4, 1, 5, 7]  \n11                           [6, 39, 8, 40, 1, 41, 7]  \n12                     [6, 29, 8, 16, 8, 10, 1, 5, 7]  \n13                     [6, 42, 1, 16, 8, 10, 1, 4, 7]  \n14  [6, 44, 2, 45, 2, 13, 3, 3, 1, 46, 2, 47, 2, 1...  \n15   [6, 52, 1, 53, 8, 54, 1, 55, 1, 17, 2, 30, 3, 7]  \n16              [6, 19, 1, 20, 1, 21, 1, 15, 1, 5, 7]  \n17                [6, 12, 8, 22, 1, 9, 8, 4, 1, 5, 7]  \n18              [6, 57, 1, 58, 1, 59, 1, 60, 1, 5, 7]  \n19                     [6, 62, 1, 16, 1, 10, 1, 4, 7]  \n20                                   [6, 16, 1, 4, 7]  \n21                      [6, 29, 1, 10, 1, 4, 1, 5, 7]  \n22              [6, 63, 1, 64, 1, 65, 1, 15, 1, 5, 7]  \n23                             [6, 12, 1, 9, 1, 5, 7]  \n24                              [6, 9, 1, 4, 1, 5, 7]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>function_tokens</th>\n      <th>expansion_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[31, 2, 4, 3]</td>\n      <td>[6, 24, 1, 4, 7]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[2, 4, 1, 5, 3, 14, 2, 18, 3]</td>\n      <td>[6, 19, 8, 20, 1, 21, 8, 15, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[25, 2, 4, 3]</td>\n      <td>[6, 32, 1, 4, 7]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[11, 2, 5, 8, 4, 3]</td>\n      <td>[6, 12, 1, 22, 1, 9, 1, 4, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[23, 2, 26, 2, 4, 3, 3]</td>\n      <td>[6, 33, 1, 34, 1, 10, 1, 4, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[35, 2, 4, 3]</td>\n      <td>[6, 36, 1, 4, 7]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>[11, 2, 9, 1, 5, 3]</td>\n      <td>[6, 12, 8, 9, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>[37, 2, 4, 3]</td>\n      <td>[6, 27, 8, 10, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>[2, 4, 1, 5, 3, 14, 2, 18, 3]</td>\n      <td>[6, 19, 8, 20, 1, 21, 8, 15, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>[11, 2, 4, 1, 5, 3]</td>\n      <td>[6, 12, 8, 22, 1, 9, 8, 4, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>[23, 2, 4, 3]</td>\n      <td>[6, 27, 1, 24, 1, 10, 1, 4, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>[38, 2, 4, 3]</td>\n      <td>[6, 39, 8, 40, 1, 41, 7]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>[2, 5, 8, 4, 3, 28, 2, 4, 3]</td>\n      <td>[6, 29, 8, 16, 8, 10, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>[17, 2, 4, 1, 5, 3]</td>\n      <td>[6, 42, 1, 16, 8, 10, 1, 4, 7]</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>[17, 2, 4, 1, 5, 3, 43, 2, 13, 3]</td>\n      <td>[6, 44, 2, 45, 2, 13, 3, 3, 1, 46, 2, 47, 2, 1...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>[17, 2, 51, 1, 30, 3]</td>\n      <td>[6, 52, 1, 53, 8, 54, 1, 55, 1, 17, 2, 30, 3, 7]</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>[2, 5, 8, 4, 3, 14, 2, 18, 3]</td>\n      <td>[6, 19, 1, 20, 1, 21, 1, 15, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>[11, 2, 4, 1, 5, 3]</td>\n      <td>[6, 12, 8, 22, 1, 9, 8, 4, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>[2, 5, 8, 4, 3, 14, 2, 56, 3]</td>\n      <td>[6, 57, 1, 58, 1, 59, 1, 60, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>[61, 2, 5, 8, 4, 3]</td>\n      <td>[6, 62, 1, 16, 1, 10, 1, 4, 7]</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>[26, 2, 4, 3]</td>\n      <td>[6, 16, 1, 4, 7]</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>[23, 2, 25, 2, 4, 3, 3]</td>\n      <td>[6, 29, 1, 10, 1, 4, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>[2, 4, 1, 5, 3, 28, 2, 4, 3]</td>\n      <td>[6, 63, 1, 64, 1, 65, 1, 15, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>[11, 2, 5, 8, 9, 3]</td>\n      <td>[6, 12, 1, 9, 1, 5, 7]</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>[9, 1, 4, 1, 5]</td>\n      <td>[6, 9, 1, 4, 1, 5, 7]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"id":"c5e9ca24-9c76-4c90-82a0-a3ba52864d46","cell_type":"markdown","source":"# PyTorch Dataset and Collate Function","metadata":{}},{"id":"cbd09041-6f73-4e46-8857-46c064947880","cell_type":"code","source":"# PyTorch Dataset\nclass TrainDataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        data = self.dataset.iloc[idx]\n        # For seq2seq, we need both an encoder input and a decoder target.\n        # Here, function_tokens are encoder input; expansion_tokens are decoder target.\n        function_tensor = torch.tensor(data['function_tokens'], dtype=torch.long)\n        expansion_tensor = torch.tensor(data['expansion_tokens'], dtype=torch.long)\n        return function_tensor, expansion_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T06:38:19.442359Z","iopub.execute_input":"2025-03-21T06:38:19.442835Z","iopub.status.idle":"2025-03-21T06:38:19.450675Z","shell.execute_reply.started":"2025-03-21T06:38:19.442792Z","shell.execute_reply":"2025-03-21T06:38:19.449494Z"}},"outputs":[],"execution_count":3},{"id":"b067e4ce-689d-4fe7-b9c7-f3364bf9148e","cell_type":"code","source":"# Collate function to pad sequences\ndef collate_fn(batch):\n    # batch: list of (src, trg) pairs\n    src_seqs, trg_seqs = zip(*batch)\n    src_lengths = [len(s) for s in src_seqs]\n    trg_lengths = [len(t) for t in trg_seqs]\n    src_padded = nn.utils.rnn.pad_sequence(src_seqs, batch_first=True, padding_value=0)\n    trg_padded = nn.utils.rnn.pad_sequence(trg_seqs, batch_first=True, padding_value=0)\n    return src_padded, trg_padded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T06:38:27.195874Z","iopub.execute_input":"2025-03-21T06:38:27.196230Z","iopub.status.idle":"2025-03-21T06:38:27.201583Z","shell.execute_reply.started":"2025-03-21T06:38:27.196201Z","shell.execute_reply":"2025-03-21T06:38:27.200516Z"}},"outputs":[],"execution_count":5},{"id":"fc0a9728-ca8e-479c-a1c1-88ad6b911521","cell_type":"markdown","source":"# LSTM Encoder-Decoder based model","metadata":{}},{"id":"f1725212-236c-40c7-b1a4-e34ebc7b3302","cell_type":"code","source":"# Encoder\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, embed_size, hidden_size, num_layers):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, embed_size, padding_idx=0)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n    \n    def forward(self, src):\n        # src: [batch, src_len]\n        embedded = self.embedding(src)  # [batch, src_len, embed_size]\n        outputs, (hidden, cell) = self.lstm(embedded)\n        return hidden, cell\n\n# Decoder\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, embed_size, hidden_size, num_layers):\n        super().__init__()\n        self.output_dim = output_dim\n        self.embedding = nn.Embedding(output_dim, embed_size, padding_idx=0)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_dim)\n        self.log_softmax = nn.LogSoftmax(dim=1)\n    \n    def forward(self, input, hidden, cell):\n        # input: [batch] -> we want [batch, 1]\n        input = input.unsqueeze(1)\n        embedded = self.embedding(input)  # [batch, 1, embed_size]\n        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))  # output: [batch, 1, hidden_size]\n        prediction = self.log_softmax(self.fc(output.squeeze(1)))  # [batch, output_dim]\n        return prediction, hidden, cell\n\n# Seq2Seq Model\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n    \n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        # src: [batch, src_len]\n        # trg: [batch, trg_len]\n        batch_size = src.size(0)\n        trg_len = trg.size(1)\n        output_dim = self.decoder.output_dim\n        \n        outputs = torch.zeros(batch_size, trg_len, output_dim).to(self.device)\n        hidden, cell = self.encoder(src)\n        # first input to decoder is the <SOS> token\n        input = trg[:, 0]  # [batch]\n        \n        for t in range(1, trg_len):\n            output, hidden, cell = self.decoder(input, hidden, cell)\n            outputs[:, t, :] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)  # [batch]\n            input = trg[:, t] if teacher_force else top1\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T06:38:28.014566Z","iopub.execute_input":"2025-03-21T06:38:28.014953Z","iopub.status.idle":"2025-03-21T06:38:28.025673Z","shell.execute_reply.started":"2025-03-21T06:38:28.014919Z","shell.execute_reply":"2025-03-21T06:38:28.024823Z"}},"outputs":[],"execution_count":6},{"id":"70f8a428-9265-498a-81d2-5bc0e70b6409","cell_type":"markdown","source":"# Training Class for Transformer","metadata":{}},{"id":"a07ad1b5-f507-494d-b039-c3a219dc43aa","cell_type":"code","source":"# Training Class\nclass Train:\n    def __init__(self, epoch, batch_size, input_dim, embed_size, hidden_size, num_layers, output_dim):\n        self.epoch = epoch\n        self.batch_size = batch_size\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        encoder = Encoder(input_dim, embed_size, hidden_size, num_layers)\n        decoder = Decoder(output_dim, embed_size, hidden_size, num_layers)\n        self.model = Seq2Seq(encoder, decoder, self.device).to(self.device)\n        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n        self.criterion = nn.NLLLoss(ignore_index=0)\n    \n    def run(self, dataloader, trg_pad_idx=0):\n        for epoch in range(self.epoch):\n            self.model.train()\n            epoch_loss = 0\n            for src, trg in dataloader:\n                src, trg = src.to(self.device), trg.to(self.device)\n                self.optimizer.zero_grad()\n                output = self.model(src, trg)\n                # output: [batch, trg_len, output_dim]\n                # trg: [batch, trg_len]\n                # flatten both for loss computation:\n                output = output[:, 1:].reshape(-1, output.shape[-1])  # skip first token (<SOS>) prediction\n                trg = trg[:, 1:].reshape(-1)\n                loss = self.criterion(output, trg)\n                loss.backward()\n                self.optimizer.step()\n                epoch_loss += loss.item()\n            if (epoch + 1) % 100 == 0 or epoch == 0:\n                print(f'Epoch {epoch+1} - Loss: {epoch_loss:.4f}')\n    \n    def get_model(self):\n        return self.model.to(\"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T06:38:37.769259Z","iopub.execute_input":"2025-03-21T06:38:37.769585Z","iopub.status.idle":"2025-03-21T06:38:37.778670Z","shell.execute_reply.started":"2025-03-21T06:38:37.769560Z","shell.execute_reply":"2025-03-21T06:38:37.777452Z"}},"outputs":[],"execution_count":7},{"id":"a616a787-57d0-4712-9097-a9bcdccb3515","cell_type":"markdown","source":"# Initialize Dataset and Training Setup","metadata":{}},{"id":"45ae91ab-baff-474f-8cc3-a037e091cc6a","cell_type":"code","source":"# PyTorch Training Setup\nvocab_to_int, int_to_vocab = taylor_dataset.get_token_dicts()\ndataset = TrainDataset(tokenized_df)\ntrain_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n\n# Hyperparameters\nepoch = 500\nbatch_size = 1\nembed_size = 32\nhidden_size = 64\nnum_layers = 2\ninput_dim = len(vocab_to_int) + 1  # +1 for padding idx=0\noutput_dim = len(vocab_to_int) + 1\n\n# Train Model\ntrainer = Train(epoch, batch_size, input_dim, embed_size, hidden_size, num_layers, output_dim)\ntrainer.run(train_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T06:38:38.301256Z","iopub.execute_input":"2025-03-21T06:38:38.301577Z","iopub.status.idle":"2025-03-21T06:42:28.291171Z","shell.execute_reply.started":"2025-03-21T06:38:38.301553Z","shell.execute_reply":"2025-03-21T06:42:28.289948Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 - Loss: 102.4220\nEpoch 100 - Loss: 1.8235\nEpoch 200 - Loss: 0.1597\nEpoch 300 - Loss: 0.0348\nEpoch 400 - Loss: 0.0089\nEpoch 500 - Loss: 0.0024\n","output_type":"stream"}],"execution_count":8},{"id":"d202b832-2b65-4606-99f1-69b44d727d7b","cell_type":"markdown","source":"# Prediction function","metadata":{}},{"id":"539b2100-fe05-49e9-ba46-07160a3de502","cell_type":"code","source":"def predict_sample(model, src_tensor, vocab_to_int, int_to_vocab, max_len=30):\n    \"\"\"\n    Predicts the output sequence for a given input sequence (src_tensor) using the trained model.\n    \n    Args:\n        model: The trained Seq2Seq model.\n        src_tensor: Tensor containing the tokenized input sequence (1D tensor).\n        vocab_to_int: Dictionary mapping tokens to indices.\n        int_to_vocab: Dictionary mapping indices to tokens.\n        max_len: Maximum number of tokens to generate.\n    \n    Returns:\n        List of tokens representing the predicted expansion (without <SOS> token).\n    \"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n    \n    # Add batch dimension and send to device\n    src_tensor = src_tensor.unsqueeze(0).to(device)\n    \n    # Encode the input sequence\n    with torch.no_grad():\n        hidden, cell = model.encoder(src_tensor)\n    \n    # First decoder input is <SOS>\n    sos_token = vocab_to_int[\"<SOS>\"]\n    eos_token = vocab_to_int[\"<EOS>\"]\n    input_token = torch.tensor([sos_token], device=device)\n    \n    predicted_tokens = []\n    \n    # Decode one token at a time\n    for _ in range(max_len):\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(input_token, hidden, cell)\n        top1 = output.argmax(1).item()\n        if top1 == eos_token:\n            break\n        predicted_tokens.append(top1)\n        input_token = torch.tensor([top1], device=device)\n    \n    # Convert token indices to words\n    predicted_words = [int_to_vocab[token] for token in predicted_tokens]\n    return predicted_words\n\n# Assume 'trainer' is your training instance from the previous code and has been trained.\ntrained_model = trainer.get_model()\ntrained_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T06:42:28.292568Z","iopub.execute_input":"2025-03-21T06:42:28.292892Z","iopub.status.idle":"2025-03-21T06:42:28.307190Z","shell.execute_reply.started":"2025-03-21T06:42:28.292864Z","shell.execute_reply":"2025-03-21T06:42:28.306229Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Seq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(67, 32, padding_idx=0)\n    (lstm): LSTM(32, 64, num_layers=2, batch_first=True)\n  )\n  (decoder): Decoder(\n    (embedding): Embedding(67, 32, padding_idx=0)\n    (lstm): LSTM(32, 64, num_layers=2, batch_first=True)\n    (fc): Linear(in_features=64, out_features=67, bias=True)\n    (log_softmax): LogSoftmax(dim=1)\n  )\n)"},"metadata":{}}],"execution_count":9},{"id":"303f5e69-d9e5-4e47-8a66-a92ce8250929","cell_type":"code","source":"# Pick a sample input from the dataset (e.g., first sample)\nsample_input, sample_target = dataset[0]\n\n# Predict the expansion using the sample function tokens\npredicted_expansion = predict_sample(trained_model, sample_input, vocab_to_int, int_to_vocab)\nprint(\"Function Tokens (input):\", [int_to_vocab[token] for token in sample_input.tolist()])\nprint(\"Predicted Expansion:\", \" \".join(predicted_expansion))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T06:42:28.308849Z","iopub.execute_input":"2025-03-21T06:42:28.309181Z","iopub.status.idle":"2025-03-21T06:42:28.343599Z","shell.execute_reply.started":"2025-03-21T06:42:28.309155Z","shell.execute_reply":"2025-03-21T06:42:28.342560Z"}},"outputs":[{"name":"stdout","text":"Function Tokens (input): ['asin', '(', 'x', ')']\nPredicted Expansion: x**3/6 + x\n","output_type":"stream"}],"execution_count":10},{"id":"c82e6496-14d1-448d-9c81-0ee91a9c2e2a","cell_type":"code","source":"# Pick a sample input from the dataset (e.g., first sample)\nsample_input, sample_target = dataset[1]\n\n# Predict the expansion using the sample function tokens\npredicted_expansion = predict_sample(trained_model, sample_input, vocab_to_int, int_to_vocab)\nprint(\"Function Tokens (input):\", [int_to_vocab[token] for token in sample_input.tolist()])\nprint(\"Predicted Expansion:\", \" \".join(predicted_expansion))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T06:42:28.344812Z","iopub.execute_input":"2025-03-21T06:42:28.345189Z","iopub.status.idle":"2025-03-21T06:42:28.360588Z","shell.execute_reply.started":"2025-03-21T06:42:28.345154Z","shell.execute_reply":"2025-03-21T06:42:28.359490Z"}},"outputs":[{"name":"stdout","text":"Function Tokens (input): ['(', 'x', '+', '1', ')', '**', '(', '-2', ')']\nPredicted Expansion: 5*x**4 - 4*x**3 + 3*x**2 - 2*x + 1\n","output_type":"stream"}],"execution_count":11},{"id":"bd3f4950-f549-4175-87af-25969560a7cc","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}